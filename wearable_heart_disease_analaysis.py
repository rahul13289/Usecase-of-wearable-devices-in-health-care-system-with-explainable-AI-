# -*- coding: utf-8 -*-
"""wearable_heart_disease_analaysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HyNse4YBtnsG1N0iTFUJaS5WPAG0ZiZj
"""

#import data
import pandas as pd
df=pd.read_excel('health_data.xlsx')
df.head()

df.info()

#import data
df=pd.read_excel('health_data.xlsx',index_col=0,header=0,sheet_name=0)
df.head()

#import data
df=pd.read_excel('health_data.xlsx',usecols=[0,18])
df.head()

#data analyze
import matplotlib.pyplot as plt

#data analyze
df=pd.read_excel('health_data.xlsx')

#data analyze
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#data analyze
sns.stripplot(df['abdnormal'],df['temperature'])

#data_analyze
import plotly.express as px
import plotly

#data_analyze
df=pd.read_excel('health_data.xlsx')
values=df['age']
names=df['abdnormal']
fig=px.pie(df,values=values,names=names,title="Pie Chart")
fig.show()

#data analyze
plt.bar(df['age'],df['chol'])
plt.xlabel('age')
plt.ylabel('cholestrol')
plt.title("Bar Chart")

#data cleanining(exploring the data)
df.info()

health_data=pd.read_excel('health_data.xlsx')

#data cleaning
df.value_counts()

#data cleaning
df.dropna()

#data cleaning
df.drop(columns='cp',axis=1)

#data cleaning
df.drop(columns='Unnamed: 19',axis=1)

#data cleaning
df2=df.replace({'ca':'fbs'})
print(df2)

import seaborn as sns

sns.stripplot(df['age'],df['chol'])

sns.stripplot(df['chol'],df['heart rate'])

#data visulaize
plt.hist(df['oldpeak'])
plt.title("Histogram")

#prediction model
df.corr()

#prediction
from sklearn.linear_model import LinearRegression 
from sklearn.model_selection import train_test_split

df.shape

df.describe()

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

df.isnull().any()

df.dropna()

df.isnull().any()

df.dropna(axis=1)

df.dropna(axis=0)

#Finding the nan column
df.isnull().any()

#To delete the column in excel 
del df['Unnamed: 19']

df.isnull().any()

X= df.iloc[:,1:18]   
Y= df.iloc[:,-1]

best_features= SelectKBest(score_func=chi2, k=3)
fit= best_features.fit(X,Y)

df_scores= pd.DataFrame(fit.scores_)
df_columns= pd.DataFrame(X.columns)

features_scores= pd.concat([df_columns, df_scores], axis=1)
features_scores.columns= ['Features', 'Score']
features_scores.sort_values(by = 'Score')

X= df [['temperature','age','heart rate','chol']]
y= df[['abdnormal']]

X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.4,random_state=100)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

logreg= LogisticRegression()
logreg.fit(X_train,y_train)

y_pred=logreg.predict(X_test)
print (X_test) #test dataset
print (y_pred) #predicted values

from sklearn import metrics
from sklearn.metrics import classification_report
print('Accuracy:',metrics.accuracy_score(y_test, y_pred))
print('CL Report:',metrics.classification_report(y_test, y_pred, zero_division=1))

#Plotting regression curve and prediction curve
import matplotlib.pyplot as plt
plt.plot(X_test,y_pred,color='k')
plt.show()

#KNN Algorithm-Importing the modules
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

from sklearn.datasets import make_blobs
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

#Step 2: Importing the dataset
X, y = make_blobs(n_samples = 500, n_features = 2, centers = 4,cluster_std = 1.5, random_state = 4)

#3. Visualize the Dataset
plt.style.use('seaborn')
plt.figure(figsize = (10,10))
plt.scatter(X[:,0], X[:,1], c=y, marker= '*',s=100,edgecolors='black')
plt.show()

#4. Splitting Data into Training and Testing Datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)

from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train,y_train)
pred=knn.predict(X_test)

from sklearn.metrics import classification_report,confusion_matrix

print(confusion_matrix(y_test,pred))
print(classification_report(y_test,pred))

#5. KNN Classifier Implementation
knn50 = KNeighborsClassifier(n_neighbors = 50)
knn30 = KNeighborsClassifier(n_neighbors=30)

#6. Predictions for the KNN Classifiers
knn50.fit(X_train, y_train)
knn30.fit(X_train, y_train)

y_pred_50 = knn50.predict(X_test)
y_pred_30 = knn30.predict(X_test)

#7. Predict Accuracy for both k values
from sklearn.metrics import accuracy_score
print("Accuracy with k=50", accuracy_score(y_test, y_pred_50)*100)
print("Accuracy with k=30", accuracy_score(y_test, y_pred_30)*100)

from sklearn import metrics
from sklearn.metrics import classification_report
print('Accuracy:',metrics.accuracy_score(y_test, pred))
print('CL Report:',metrics.classification_report(y_test, pred, zero_division=1))

#8. Visualize Predictions
plt.figure(figsize = (15,5))
plt.subplot(1,2,1)
plt.scatter(X_test[:,0], X_test[:,1], c=y_pred_50, marker= '*', s=100,edgecolors='black')
plt.title("Predicted values with k=50", fontsize=20)

plt.subplot(1,2,2)
plt.scatter(X_test[:,0], X_test[:,1], c=y_pred_30, marker= '*', s=100,edgecolors='black')
plt.title("Predicted values with k=30", fontsize=20)
plt.show()

sns.pairplot(df,hue='abdnormal',palette='coolwarm')

#Step 1: Importing the Libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

dataset = pd.read_excel('health_data.xlsx')
#X = dataset.iloc[:, [0, 1]].values
#y = dataset.iloc[:, 2].values
X, y = make_blobs(n_samples = 500, n_features = 2, centers = 4,cluster_std = 1.5, random_state = 4)
dataset.head(5)

del dataset['Unnamed: 19']

#Step 2: Importing the dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)

#Step 3: Splitting the dataset into the Training set and Test set
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#Step 4: Feature Scaling
from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf', random_state = 0)
classifier.fit(X_train, y_train)

#Step 5: Training the SVM Classification model on the Training Set
y_pred = classifier.predict(X_test) 
y_pred

#Step 7: Confusion Matrix and Accuracy
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
from sklearn.metrics import accuracy_score 
print ("Accuracy : ", accuracy_score(y_test, y_pred))
print('CL Report:',metrics.classification_report(y_test, y_pred, zero_division=1))
print(cm)

#Step 8: Comparing the Real Values with Predicted Values
df = pd.DataFrame({'Real Values':y_test, 'Predicted Values':y_pred})
df

#Step 9: Visualizing the Results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('SVM Classification')
plt.xlabel('Age')
plt.ylabel('adnormal')
plt.legend()
plt.show()





#Comparison of accuracy in ml algorithms
import numpy as np
import matplotlib.pyplot as plt
# creating the dataset
data = {'Without GPU':73, 'With gpu':96.8, 'SVM':93}
algorithm = list(data.keys())
values = list(data.values())
sns.set(rc={'figure.figsize':(15,8)})
plt.xlabel("Algorithms")
plt.ylabel("Accuracy score")

sns.barplot(algorithm,values)
#fig = plt.figure(figsize = (10, 5))

# creating the bar plot
#plt.bar(algorithm, values, color ='maroon',
#		width = 0.4)

#plt.xlabel("Algorithms used")
#plt.ylabel("Accuracy for algorithm")
#plt.title("Comparison of machine learning algorithms used")
#plt.show()

!pip install shap

#Using shap model
import shap

shap.initjs()

import pandas as pd
import numpy as np
import plotly
np.random.seed(0)
import matplotlib.pyplot as plt

df=pd.read_excel('health_data.xlsx')

df.shape

#Making use of SHAP in explainable ai
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor

X= df[['temperature', 'o2', 'blood pressure']] 
Y = df['chol']

X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.4,random_state=100)

model = RandomForestRegressor(max_depth=6, random_state=0, n_estimators=10)

model.fit(X_train,y_train)

print(model.feature_importances_)

shap_values = shap.TreeExplainer(model).shap_values(X_train)

shap.summary_plot(shap_values, X_train, plot_type="bar")

import matplotlib.pyplot as plt

# Get the predictions and put them with the test data.
X_output = X_test.copy()
X_output.loc[:,'predict'] = np.round(model.predict(X_output),2)

# Randomly pick some observations
#syntax for arange(start,stop,step)
random_picks = np.arange(1,30,50) # Every 50 rows 
S = X_output.iloc[random_picks]
S

# Initialize your Jupyter notebook with initjs(), otherwise you will get an error message.
shap.initjs()

# Write in a function
def shap_plot(j):
    explainerModel = shap.TreeExplainer(model)
    shap_values_Model = explainerModel.shap_values(S)
    p = shap.force_plot(explainerModel.expected_value, shap_values_Model[j], S.iloc[[j]])
    return(p)

X_train.mean()

df=pd.read_excel('health_data.xlsx')
X= df[['temperature', 'o2', 'blood pressure']]  
Y= df[['age']]

X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.4,random_state=100)

regressor = RandomForestRegressor()
regressor.fit(X_train, y_train);

# Create object that can calculate shap values
explainer = shap.TreeExplainer(regressor)
# Calculate Shap values
shap_values = explainer.shap_values(X_train)

print(shap_values)

import shap  

explainer = shap.TreeExplainer(rfc)

# calculate shap values. This is what we will plot.
shap_values = explainer.shap_values(X_test)



#Making use of LIME concept in explainabel ai
!pip install lime

import numpy as np
import pandas as pd

df = pd.read_excel('health_data.xlsx')
df.head()

del df['Unnamed: 19']

from sklearn.model_selection import train_test_split

X= df[['age','blood pressure','chol','heart rate','o2','temperature','thalach','trestbps']] 
y = df['abdnormal'] 

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)
score = model.score(X_test, y_test)

import lime
from lime import lime_tabular

explainer = lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train),
    feature_names=X_train.columns,
    class_names=['bad', 'good'],
    mode='classification'
)

"""# Explainable ai concept using local interept model agostic explain

The random forestclassifier was implemented for the prediction purpose.The input parameter for X are temperature,blood pressure and O2 level.
The input parameter for y are abdnormal.
The iloc() method was used to select particular column or row from the data. 

"""

exp = explainer.explain_instance(
    data_row=X_test.iloc[10], 
    predict_fn=model.predict_proba
)

exp.show_in_notebook(show_table=True)

exp = explainer.explain_instance(
    data_row=X_test.iloc[4], 
    predict_fn=model.predict_proba
)

exp.show_in_notebook(show_table=True)

exp = explainer.explain_instance(
    data_row=X_test.iloc[20], 
    predict_fn=model.predict_proba
)

exp.show_in_notebook(show_table=True)

import matplotlib.pyplot as plt

import pandas as pd
df=pd.read_excel('health_data.xlsx')

!pip install interpret

!pip install sklearn

#ExplainableBoostingRegressor
from interpret.glassbox import ExplainableBoostingRegressor
from sklearn.datasets import load_boston

X, y = load_boston(return_X_y=True)

ebm = ExplainableBoostingRegressor()
ebm.fit(X, y)

from interpret import show

show(ebm.explain_global())

import shap

ex = shap.TreeExplainer(model)
shap_values = ex.shap_values(X_test)
shap.summary_plot(shap_values, X_test)

#Explainable boosting machine
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

#interpretml 
from interpret import show
from interpret.data import Marginal
from interpret.glassbox import ExplainableBoostingRegressor, LinearRegression, RegressionTree

seed = 1
train = pd.read_excel('health_data.xlsx')
test = pd.read_excel('health_data.xlsx')

columns= ['age','blood pressure','chol','heart rate','o2','temperature','thalach','trestbps']
target = ['target'] 

X_train, X_test, y_train, y_test = train_test_split(train[columns],train[target], test_size=0.20)

#train ebm
ebm = ExplainableBoostingRegressor(random_state=seed, n_jobs=-1)
ebm.fit(X_train,y_train)

#Evalvating EBM performance
from interpret.perf import RegressionPerf

ebm_perf = RegressionPerf(ebm.predict).explain_perf(X_test, y_test, name='EBM')
show(ebm_perf)

#Comparing EGB performance with other regressors
lr = LinearRegression(random_state=seed)
lr.fit(X_train, y_train)

rt = RegressionTree(random_state=seed)
rt.fit(X_train, y_train)

rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)
rf.fit(X_train, y_train)

#Comparing the performance of different models
lr_perf = RegressionPerf(lr.predict).explain_perf(X_test, y_test, name='Linear Regression')
rt_perf = RegressionPerf(rt.predict).explain_perf(X_test, y_test, name='Regression Tree')
rf_perf = RegressionPerf(rf.predict).explain_perf(X_test, y_test, name='Blackbox')
lr_global = lr.explain_global(name='Linear Regression')
rt_global = rt.explain_global(name='Regression Tree')

#Comparing the performance of different models
show(lr_perf)
show(rt_perf)
show(ebm_perf)
show(rf_perf)

#Lime implement
# Importing the necessary libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Loading the dataset using sklearn
from sklearn.datasets import load_boston
data = load_boston()

# Displaying relevant information about the data
print(data['DESCR'][200:1420])

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report

# Our algorithms, by from the easiest to the hardest to intepret.
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost.sklearn import XGBClassifier

# Separating data into feature variable X and target variable y respectively
from sklearn.model_selection import train_test_split
X = df['temperature']
y = df['target']

# Extracting the names of the features from data
features = data['feature_names']

# Splitting X & y into training and testing set
X_train, X_test, y_train, y_test = train_test_split(
	X, y, train_size=0.90, random_state=50)

# Creating a dataframe of the data, for a visual check
df = pd.concat([pd.DataFrame(X), pd.DataFrame(y)], axis=1)
df.columns = np.concatenate((features, np.array(['target'])))
print("Shape of data =", df.shape)

# Printing the top 5 rows of the dataframe
df.head()

# Timing Python Scripts

# Import relevant modules
import time
from datetime import datetime

# Start of the python script
script_start_time = datetime.now()

# Extra coding...
for i in range(10000000):
    j = 10 + i

# Timing a section of python code
# This gives the start time
start_time = time.time()
# Do some code
for i in range(10000000):
    j = 10 + i
# End time once the code has finished
end_time = time.time()
# Total time
final_time = end_time - start_time
print(final_time)

# Script end time
script_end_time = datetime.now()
print(script_end_time - script_start_time)



import psutil
N_physical_cores = psutil.cpu_count(logical=False)
N_logical_cores = psutil.cpu_count(logical=True)
print(f"The number of physical/logical cores is {N_physical_cores}/{N_logical_cores}")

import multiprocessing as mp

print('Number of cores:',mp.cpu_count())

import time as time
import pandas as pd

data=pd.read_excel('health_data.xlsx')